"""
Three Python Classes:
    1. ClassesGenerator
        Takes as input a JSON generated by doc_decoder_oo.py
        Generates a list of classes using GPT3.5
        Add them as a new subsection of the JSON
    2. SentenceClassifier
        Takes as input a JSON tagged by ClassesGenerator
        Classifies each sentence
        Adds the classification to the JSON
    3. ChunkFinder
        Takes as input a class name
        Finds all sentences or chunks or documents tagged with that class by SentenceClassifier
"""
import copy
import os
import random
import sys
from pprint import pprint
import warnings
warnings.simplefilter(action='ignore', category=Warning)
# Modeling
from transformers import pipeline
# download for cpu
one_shot_classifier = pipeline(task="zero-shot-classification",
                      model="facebook/bart-large-mnli",
                        device=-1)  # -1 for cpu, 0 for gpu

from openai import OpenAI


import json

from keys import KEY
client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key=KEY,
)


class ClassesGenerator:
    def __init__(self, document_filename: str):
        self.document_filename = document_filename
        self.document_json_filename = self.document_filename + ".json"
        self.max_tokens = 4096
        # * 0.8 is needed for reasons yet to be understood
        self.max_words = int(0.7 * self.max_tokens * 0.8)
        self.classes_prompt = """Please generate 10 single-word labels to classify the sentences across the following text. 
                    Do not try to classify them yet. Just come up with 10 useful classes:"""


    def send_prompt(self, prompt, model="gpt-3.5-turbo"):
        completions = client.chat.completions.create(
            model= model,
            messages=[
                {"role": "user",
                 "content": prompt},
            ]
        )
        #message = completions.choices[0]['message']['content']
        message = completions.choices[0].message.content
        return message

    def generate_document_string(self, json):
        document_string = ""
        document_type = json['document_chunks']['document_type']
        # the JSON has slightly different structures for different document types
        if document_type == 'pdf':
            for page in json['document_chunks']['content']['pages']:
                document_string += page['text'] + "\n"
        elif document_type == 'pptx':
            for slide in json['document_chunks']['content']['slides']:
                for shape in slide['shapes']:
                    document_string += shape['text'] + "\n"
        elif document_type == 'xlsx':
            for sheet in json['document_chunks']['content']['sheets']:
                document_string += sheet['sheet_name'] + "\n"
                for col in sheet['columns']:
                    document_string += ",".join(col) + "\n"
        elif document_type == 'docx':
            for title, paragraph in json['document_chunks']['content']['titled_subtitled_paragraphs'].items():
                document_string += title + "\n" + paragraph + "\n"
        return document_string

    def generate_classes(self, document_string):
        # cut off the end of the document string if it's greater than the max words
        if len(document_string.split()) > self.max_words:
            document_string = " ".join(document_string.split()[:self.max_words])
        prompt = self.classes_prompt + "\n" + document_string
        response = self.send_prompt(prompt)
        return response

    # main method, which converts the original document json to
    # json with a (sentence-based) 'classes' section using GPT3.5
    def generate_classes_json(self):
        # load in the json file
        with open(self.document_json_filename, 'r') as f:
            document_json = json.load(f)
        document_string = self.generate_document_string(document_json)
        classes = self.generate_classes(document_string)
        # classes in in GPT3.5 NLP format, so convert to a list
        classes = classes.split("\n")
        for i, c in enumerate(classes):
            if len(c.split()[0]) == 2:
                classes[i] = c[3:]
            else:
                classes[i] = c[4:]
        document_json['document_chunks']['classes'] = classes
        with open(self.document_filename+"_classed.json", 'w') as f:
            json.dump(document_json, f, indent=4)
        return document_json

class SentenceClassifier:
    def __init__(self, document_filename: str):
        self.document_filename = document_filename
        self.document_json_filename = self.document_filename + "_classed.json"

    def one_shot_classification_old(self, sentence, classes):
        return random.choice(classes), int(100*random.random())

    def one_shot_classification(self, sentence, classes):
        sentence = sentence.strip()
        if not sentence:
            return "no_sentence", 0
        # Set the prompt
        print("Predicting class for sentence '", sentence, end="': ")
        prompt = "The topic of this sentence is {}."
        single_topic_prediction = one_shot_classifier([sentence], classes,
                                             hypothesis_template=prompt)
        labels = single_topic_prediction[0]["labels"]
        scores = single_topic_prediction[0]["scores"]
        sorted_labels = [label for _, label in sorted(zip(scores, labels), reverse=True)]
        sorted_scores = [score for score, _ in sorted(zip(scores, labels), reverse=True)]
        print(f"{sorted_labels[0]} ({sorted_scores[0]:.2f})")
        return sorted_labels[0], int(100*sorted_scores[0])


    def get_classes(self, sentences, classes):
        classifications = []
        confidences = []
        for s in sentences:
            class_pred, pred_confidence = self.one_shot_classification(s, classes)
            if class_pred == "no_sentence":
                classifications.append(-1)
                confidences.append(0)
                continue
            class_pred_index = classes.index(class_pred)
            classifications.append(class_pred_index)
            confidences.append(pred_confidence)
        return classifications, confidences


    def generate_document_sentences(self, doc_json):
        document_string = ""
        document_type = doc_json['document_chunks']['document_type']
        # the JSON has slightly different structures for different document types
        if document_type == 'pdf':
            for p, page in enumerate(doc_json['document_chunks']['content']['pages']):
                sentences = page['text'].split(".")
                classifications, confidences = self.get_classes(sentences, doc_json['document_chunks']['classes'])
                doc_json['document_chunks']['content']['pages'][p]['class_by_sentence'] = classifications
                doc_json['document_chunks']['content']['pages'][p]['sentence_class_confidence'] = confidences
        elif document_type == 'pptx':
            for slide_i, slide in enumerate(doc_json['document_chunks']['content']['slides']):
                for shape_i, shape in enumerate(slide['shapes']):
                    sentences = shape['text'].split(".")
                    classifications, confidences = self.get_classes(sentences, doc_json['document_chunks']['classes'])
                    doc_json['document_chunks']['content']['slides'][slide_i]['shapes'][shape_i]['class_by_sentence'] = classifications
                    doc_json['document_chunks']['content']['slides'][slide_i]['shapes'][shape_i]['sentence_class_confidence'] = confidences
        elif document_type == 'xlsx':
            for sheet in doc_json['document_chunks']['content']['sheets']:
                document_string += sheet['sheet_name'] + "\n"
                for col in sheet['columns']:
                    document_string += ",".join(col) + "\n"
        elif document_type == 'docx':
            new_json = copy.deepcopy(doc_json)
            for p, (title, paragraph) in enumerate(doc_json['document_chunks']['content']['titled_subtitled_paragraphs'].items()):
                sentences = paragraph.split(".")
                classifications, confidences = self.get_classes(sentences, doc_json['document_chunks']['classes'])
                para_key = list(doc_json['document_chunks']['content']['titled_subtitled_paragraphs'].keys())[p]
                new_json['document_chunks']['content']['titled_subtitled_paragraphs']['classes_by_sentence_'+para_key] = classifications
                new_json['document_chunks']['content']['titled_subtitled_paragraphs']['sentence_class_confidence_'+para_key] = confidences
            doc_json = new_json
        return doc_json

    def get_all_classes(self):
        # go through every json in json_files
        # and get all the classes into a set
        all_classes = set()
        for filename in os.listdir("json_files"):
            if filename.endswith(".json"):
                with open("json_files/"+filename, 'r') as f:
                    doc_json = json.load(f)
                all_classes.update(doc_json['document_chunks']['classes'])
        return all_classes


if __name__ == "__main__":
    # read in prompt.txt
    with open("prompts.txt", 'r') as f:
        prompt = f.read()
    new_model = "gpt-4-1106-preview"
    """cg = ClassesGenerator("dummy.docx")
    result = cg.send_prompt(prompt, new_model)
    print(result)"""
    #sys.exit()
    #filename = ".../json_files/19. Customer Touch Point gaps.pptx"
    #filename = ".../json_files/CX Methodology.pdf"
    # Write the file name here
    filename = "json_files/Executive Summary.pdf"
    cg = ClassesGenerator(filename)
    json_with_classes = cg.generate_classes_json()
    # write to file
    with open(filename+"_classed.json", 'w') as f:
        json.dump(json_with_classes, f, indent=4)
    sc = SentenceClassifier(filename)
    json_with_classified_sentences = sc.generate_document_sentences(json_with_classes)
    #cg = ClassesGenerator(".../json_files/CX Methodology.pdf")
    #cg.generate_classes_json()
    #sc = SentenceClassifier(".../json_files/CX Methodology.pdf")
    #sc = SentenceClassifier(".../json_files/HBU and EBU Customer Journey Data_v1.xlsx")
    # read in json file
    # write json to file
    with open(sc.document_filename+"_classed_sentences.json", 'w') as f:
        json.dump(json_with_classified_sentences, f, indent=4)

    """all_classes_in_dir = sc.get_all_classes()
    print("All classes in directory:")
    for i, c in enumerate(all_classes_in_dir):
        print(f"{i+1}. {c}")
    print("")
    class_index = int(input("Enter class index to view all docs with those classes: "))
    class_name = list(all_classes_in_dir)[class_index-1]
    print(f"Class selected: {class_name}")
    # now get all filenames of documents containing that class
    filenames = []
    for filename in os.listdir("json_files"):
        if filename.endswith(".json"):
            with open("json_files/"+filename, 'r') as f:
                doc_json = json.load(f)
            try:
                class_index = doc_json['document_chunks']['classes'].index(class_name)
            except ValueError:
                continue"""

